{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An introduction to data and statistical analysis ##\n",
    "\n",
    "I was asked to present an introduction to \"data\" and statistics. In two hours I am going to fail at anything comprehensive. Instead I will provide you with some things to look for, some issues that you'll come up against, and a little introduction to concepts that will help you read simple research papers. \n",
    "\n",
    "First, at the highest level, we have the unit of a data set. This is a collection of information about some phenomenon in the world. Data sets are social creatures and like to play with others, creating new data out of old through combination. Add your credit data to your web browsing behavior and we can serve you all manner of pointed ads. When you acquire a data set there are a few high-level questions you want to get right from the outset.\n",
    "\n",
    "1. Who collected these data?\n",
    "\n",
    "\n",
    "2. What were the motivations and incentives for the group that collected the data?\n",
    "\n",
    "\n",
    "3. Why were they collected originally? \n",
    "\n",
    "\n",
    "4. What methodology was used to assemble the information?\n",
    "\n",
    "\n",
    "5. Were you given the complete data set or just a portion?\n",
    "\n",
    "\n",
    "6. How have the data been used previously? And were the \"results\" published? Peer reviewed? Did they appear in a good journal or lead to some valuable insights?\n",
    "\n",
    "\n",
    "7. Were the data made public under some kind of publication standard? Did they adhere to that standard and what burden does this place on your use? \n",
    "\n",
    "\n",
    "8. What format are the data in? Are they a spreadsheet (CSV) or in XML or JSON? Is some speciality format used, and if so, do you have the software you need to analyze it? (Take as an example [the BRFSS](https://www.cdc.gov/brfss/data_documentation/index.htm).)\n",
    "\n",
    "\n",
    "9. Others?\n",
    "\n",
    "\n",
    "Data from the kinds of efforts we are going to see today come in two broad categories - **observational or experimental**. As its name suggests, in an observational study we just record what we see. We talk to people or measure something, but we don't affect them in any other way. In an experimental setting, we introduce different \"treatments\" to see if groups differ based on the treatment they receive. Here you can think of A/B testing as a classic example - visitors to a web site are shown one of two headlines and the publication decides which is better by seeing if the click-through rate is higher for one over the other. \n",
    "\n",
    "Data and statistical analyses also serve different narrative ends. In the treatment that follows, think about the kinds of methods of data collection that are in play and the kinds of conclusions that can be drawn. The veracity of our statements depend on how the data were collected. Underlying all of this is whether or not you can say something reasonable about **the uncertainty in your data**, or about your conclusions. In some cases, it will be hard to quantify uncertainty and in others, especially those based on classic statistical designs like random sampling or random allocation, we can make very precise statements about uncertainty. Ultimately, this all gets at how knowledge is created from data.\n",
    "\n",
    "### A case study: Death toll in Puerto Rico from Hurricane Maria ###\n",
    "\n",
    "Meet Rafa Irizarry. He's a statistician at Harvard University in the School of Public Health and the Dana Farber Cancer Institute. He received a Bachelor's degree from the University of Puerto Rico and went on to receive a Ph.D. in Statistics from the University of California, Berkeley. You can read about his research and teaching efforts on his [github page](http://rafalab.github.io/pages/about.html). \n",
    "\n",
    "<img src=https://pbs.twimg.com/profile_images/631957216254423040/aYpkFMFG_400x400.jpg width=200 align=left><BR CLEAR=\"LEFT\"> \n",
    "\n",
    "In September, Rafa began studying the impact of Hurricane Maria on Puerto Rico, specifically attempting to determine the number of people who perished as a result of the storm. He wrote a [blog post on the troubles he had getting access to data](https://simplystatistics.org/2018/09/28/the-complex-process-of-obtaining-puerto-rico-mortality-data-a-timeline/). His timeline is a sad tale of one researcher's struggle to get a clear sense of the damage left by the storm.\n",
    "\n",
    "We will use his post as a kind of backbone for discussing data, statistics and the ways in which data can help us learn about the world. I want you to notice the creativity that has gone into **finding ways to reveal the toll of Maria on human life.** There is a persistent questioning spirit that is on show in his post. \n",
    "\n",
    "This material is available at [http://brwn.co/breakfasts](http://brwn.co/breakfasts).\n",
    "\n",
    "#### A first look ####\n",
    "\n",
    "Rafa's chronology begins with the hurricane itself and President Trump's reaction. The death toll was quoted at 16. This struck Rafa as odd, as he was getting reports from home about electricity outages and the lack of clean water. The low death toll suggested that the local government was in control of the situation which, if incorrect, could put people at risk. Most of the text below comes from Rafa's blog post and are in <font color=\"grey\">*grey*</font>.\n",
    "\n",
    "<font color=\"grey\">**Sep 20** – Hurricane María makes landfall in Puerto Rico.\n",
    "<br><br>\n",
    "**Sep 20-30** – Reports from friends and family point to a dire situation. None of them have power, few have running water, and some have no way to communicate.\n",
    "<br><br>\n",
    "**Oct 03** – The president of the USA visits Puerto Rico. In a press conference the governor states that the death count is 16.\n",
    "<br><br>\n",
    ">“Every death is a horror,” Trump said, “but if you look at a real catastrophe like Katrina and you look at the tremendous – hundreds and hundreds of people that died – and you look at what happened here with, really, a storm that was just totally overpowering … no one has ever seen anything like this.” “What is your death count?” he asked as he turned to Puerto Rico Gov. Ricardo Rosselló. “17?” “16,” Rosselló answered. “16 people certified,” Trump said. “Sixteen people versus in the thousands. You can be very proud of all of your people and all of our people working together.\n",
    "\n",
    "**Nov 08** –  The New York times publishes an article based on funeral home data confirming our suspicion that the situation in Puerto Rico is much worse than was reported on October 3. </font> <br><br>\n",
    "Below is the NYT article. Their approach is simple year-over-year subtraction. Are there any issues with this?\n",
    "</font>\n",
    "\n",
    "[Puerto Rico Deaths Spike, but Few Are Attributed to Hurricane](https://www.nytimes.com/2017/11/08/us/puerto-rico-deaths-fema.html)\n",
    ">On Wednesday, Puerto Rico officials, facing increasing questions about the accuracy of the official death toll from the storm, acknowledged for the first time that 472 more people died this September compared with the same month last year. The storm made landfall on Sept. 20. The government’s official death toll is 55...<br><br>\n",
    "At the news conference on Wednesday, Wanda Llovet, the director of Puerto Rico’s demographics registry, said more people died in September because so many young people had left the island, leaving Puerto Rico with an aging population.\n",
    "\n",
    "<font color=\"grey\">**Nov 14** – Not having any luck finding official government data online I email the Puerto Rico Institute of Statistics (PRIS) asking for help.\n",
    "\n",
    "**Nov 16** – PRIS replies that the Demographic Registry has these data. They email the directory of the Demographic Registry on our behalf.\n",
    "\n",
    "**Nov 20** – CNN publishes an article... [and] estimate about 499 excess deaths. </font>\n",
    "\n",
    "[Here is the CNN article.](https://www.cnn.com/2017/11/20/health/hurricane-maria-uncounted-deaths-invs/index.html) Skim it and tell me what they did.\n",
    "\n",
    "I spoke with John Sutter and he said that CNN had a number of reporters on the ground and they started hearing from hospitals and funeral homes that there were a lot of people dying. Since the government was not producing data, where could they go? \"Funeral homes were the only ones with records, records that were not being processed quickly enough.\" The acquired a full list of funeral homes. They hired 5 freelancers and held a large training session in San Juan. They designed a questionnaire so that data collection was consistent - it included questions about their log of deaths, total numbers, number that they considered associated with the hurricane, why they thought that, specific examples, along with standard information like contact name/title/on record confirmation. In all, it took 2 weeks to create their own paper trail.\n",
    "\n",
    "In terms of evidence 112 or about half of the homes responded. In Sutter's words, their estimate was meant as a kind of \"gut check\" on the government's numbers. \"Did the number seem correct? Or was it an order of magnitude off?\" While carried out systematically, it is hard to do much more with this number. That is, it is hard to know what it's uncertainties might be. What could the other half of the funeral homes contribute? The funeral directors are not doctors and their determination of whether a death was storm-related was subjective. It is, as Sutter suggests, a gut check that could lead you to other analysis.\n",
    "\n",
    "<font color=\"grey\">**Nov 21** – The Penn State University (PSU) study with the first estimate of excess deaths based on mortality data is posted on SocArXiv They estimate about 1,000 excess deaths. This estimate is based on historical data from 2010-2016 and a count for September 2017 that the authors obtained from a public statement made by Puerto Rico’s Public Security Secretary.\n",
    "\n",
    "**Nov 27** - I email the corresponding author of the PSU study asking if they have the data.\n",
    "\n",
    "**Dec 03** - I scrape the data from PSU study, as described in an earlier post. This data helps guide our study design. Here is a plot of the data that clearly shows that there are more deaths than expected. The plot includes a count for October which is publicized later (see Dec 07 entry).</font>\n",
    "\n",
    "Here is [a \"preprint\" of the paper that Rafa is referring to](https://osf.io/preprints/socarxiv/s7dmu/). Papers are frequently posted to sites like [arXiv](arXiv.org) who allow researchers to share papers before they've gone through peer review. arXiv, for example, posts to its site a \"Disclaimer: Papers will be entered in the listings in order of receipt on an impartial basis and appearance of a paper is not intended in any way to convey tacit approval of its assumptions, methods, or conclusions by any agent (electronic, mechanical, or other). We reserve the right to reject any inappropriate submissions.\" When working with expert material, it is important to understand its status. Has it been subjected to peer review? Is the journal of reasonable quality?\n",
    "\n",
    "**Scraping detour**\n",
    "\n",
    "I've also made [a PDF version.](https://github.com/cocteau/pr/raw/master/papers/Excess%20mortality%20due%20to%20Hurricane%20Maria%20to%20be%20measured%20using%20a%20simple%20method.pdf) When Rafa refers to \"scraping the data,\" he means a process whereby data, stored in a format more suitable for presentation, are extracted and put in more regular format. Web pages are written in HTML and are designed for display in web browsers. PDF (the Portable Document Format) is really good for representing data to a printer -- creating a standard like this was a huge deal when PDF was introduced. Often, however, it becomes a vehicle for publishing data and then you start to see its weaknesses. Extracting data from a PDF will depend, first, on the kind of PDF it is. If you can \"select\" text from the document, you're in luck. There are several tools to help you get reasonably close to the data. If, on the other hand, your pages are essentially images, well, then you have to rely on \"optical character recognition\" programs that cluster black and white pixels, say, into letters and words and sentences.\n",
    "\n",
    "I used [Tabula](http://tabula.technology/) to pull the data from the report and convert it to something more usable, a CSV (comma separated values) file. CSV files are a popular format for tabular data. Each line in the file represents a different person or object or \"entity.\" Measurements, or \"attributes\" on a given entity occupy one line in the file and are separated by commas. You can read more about the format [here](https://en.wikipedia.org/wiki/Comma-separated_values). For those of you more familiar with a spreadshet program, a CSV will always open.\n",
    "\n",
    "Rafa's process is [documented here](https://simplystatistics.org/2017/12/03/data-do-not-agree-with-hurricane-official-death-count/) - I just followed along. As I indicated, a program like [Tabula](https://tabula.technology/) can help us create a more usable data set from this paper. Demonstration! First you load up your PDF and then you select the area of the PDF with table data. Here we highlight the main mortality data Rafa was after.\n",
    "<br><br>\n",
    "\n",
    "<img src=https://github.com/cocteau/pr/raw/master/images/tab1.jpg width=500>\n",
    "<br><br>\n",
    "\n",
    "We then convert it to a more usable file, a CSV. CSV stands for Comma Separated Values.\n",
    "<br><br>\n",
    "\n",
    "<img src=https://github.com/cocteau/pr/raw/master/images/tab2.jpg width=500>\n",
    "<br><br>\n",
    "\n",
    "I exported this file and loaded it onto github so you can download it and look at it. It is suitable for loading into Excel and all manner of data programs. [Here is the CSV.](https://github.com/cocteau/breakfasts/raw/master/data/tabula-Excess%20mortality%20due%20to%20Hurricane%20Maria%20to%20be%20measured%20using%20a%20simple%20method.csv) And below we read it into a program called R. For now, ignore the code -- I can explain it one-on-one or if there is broader interest. For now, it's a tool to make graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality <- read.csv(\"https://github.com/cocteau/breakfasts/raw/master/data/tabula-Excess%20mortality%20due%20to%20Hurricane%20Maria%20to%20be%20measured%20using%20a%20simple%20method.csv\")\n",
    "mortality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The researchers at PSU then use a slim official statistic about the deaths in September of 2018, released in a November comment. This single point gave them some indication something was wrong.\n",
    "\n",
    "[Researchers raise new questions about the hurricane death toll in Puerto Rico](https://www.cnn.com/2017/11/29/health/demographers-puerto-rico-death-toll-estimate-invs/index.html)*\n",
    ">The figures for previous years were provided by the Puerto Rico Vital Statistics system. The September 2017 figure came from the Department of Public Safety, which has said 2,838 died that month, with 95% of all deaths being reported, according to the analysis posted online. The researchers estimated the total number of deaths for September and then compared that to the averages for the same month in previous years. \"I'm very sure of the number,\" Santos said by phone.\n",
    "\n",
    "**R Detour**\n",
    "\n",
    "Now, let's start looking at R. First, you can think of R as a big calculator (a famous statistician R.A. Fisher once said that he learned all he knew over his calculator) and its simplest commands are just arithmetic expressions. Add 2 and 3 below by clicking on the cell and then hit \"enter\" while holding down the shift key -- this sends the R expression \"2+3\" to be evaluated and the answer should be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5*10\n",
    "20*4+3*sqrt(10)\n",
    "(100+log(25))/300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the cell below to enter 3 more arithmetic expressions and execute them. The top of the cell has a \"comment\" -- in R, any text that starts with a # is ignored and is another way for you to add comments to your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your expressions below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symbol `<-` is called the assignment operator. It's a bit of an old-school carry over in that back when S was developed, the APL and AT&T Terminal keyboards each had a single key to express this arrow. Jupyter's R kernel simulates this with a *keyboard shortcut* that on a Mac is Option-minus. \n",
    "\n",
    "Using the table above, use a simple calculation to tell me something about the loss of life associated with hurricane Maria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, data analysis rarely deals in single values or even a handful of values. Instead we often deal with \"structured\" data, the most popular structure being that of a simple table like we have above. Think an Excel spreadsheet. To pull the mortality data into R, we used a \"function.\" **Everything you execute in R, every command, involves executing a function in some way.** Through functions, we encapsulate operations that we have to perform repeatedly. In the cell above, we used the function `read.csv()` which does what you might expect -- It reads data in a CSV file.\n",
    "\n",
    "You can also ask for help about any function in R using, well, another function called `help()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(read.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this help page, you see how you can read in a data set and the different \"arguments\" that might alter the way the data are interpreted or the operation is performed. Above we called `read.csv()` with only the first, the URL of the file to read. \n",
    "\n",
    "Below we have created a slightly different version of the file and read it with the command `load()`. We did this because we wanted to preserve the order of the months rather than have their names sorted alphabetically. This is entirely too technical. For now, it's just a second way to load data in, this time data stored in R's own format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(url(\"https://github.com/cocteau/breakfasts/raw/master/data/mortality2.Rda\"))\n",
    "mortality2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both this table and the one above contain the same data. They just structure it slightly differently. We will have more to say about the choices later. \n",
    "\n",
    "Out of the box, R comes with a large number of things it can do. It was designed to support statistical work and so it is most expressive when it comes to statistical computations. One of the powerful things about R is the vast community that is currently supporting its development. You can read about updates on the [R site](https://www.r-project.org/) and, in particular, download new tools from a local host of [The Comprehensive R Archive Network](http://cran.stat.ucla.edu/). Here you will see *packages* or additions to base R that implement some of the most current statistical, machine learning (AI) procedures as well as \"oldies but goodies.\" A package consists of new commands and new data that someone has decided to share.\n",
    "\n",
    "Here we will make use of two packages, both written by [Hadley Wickham](http://hadley.nz/), Chief Scientist at RStudio. The first helps us manipulate tables and is called `dplyr`, and the second makes pretty nice graphics and is called `ggplot2` (the \"gg\" for [Grammar of Graphics](https://www.amazon.com/Grammar-Graphics-Statistics-Computing/dp/0387245448)). To use the new functions Hadley has written you simply use the `library()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package `dplyr` lets you do things like `filter()` the rows of a table, keeping only those that satisfy certain conditions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(mortality2,year==2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(mortality2,month==\"September\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we wanted, we could take this smaller table of September's data and create the mean or average number of deaths across every September from 2010 to 2016. Here we take the output of one function, `filter()`, and feed it as the input to another, `summarize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(filter(mortality2,month==\"September\"),avg_deaths=mean(deaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or instead of filtering by months, we can `group_by()` or \"group the data by\" `month` and then form our averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(group_by(mortality2, month), avg_deaths = mean(deaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a teaser, we take a sequence of commands where the output of one feeds as input to the next and simplify the expressions using pipes. Here the data `mortality2` are passed as input to `groupby()` and the groups are then passed as input to `summarize()`. But now we are in the weeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality2 %>%\n",
    "    group_by(month) %>%\n",
    "    summarize(avg_deaths = mean(deaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To \"see\" these data we can use the command `ggplot()` which we will get really good at. It builds up a plot from basic commands, using simple addition to add components like lines and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=10, repr.plot.height=5)\n",
    "\n",
    "ggplot() + \n",
    "    geom_line(aes(x=month,y=deaths,group=year),mortality2,color=\"grey\") +\n",
    "    geom_point(aes(x=9, y=2838), color=\"black\") +\n",
    "    geom_text(aes(x=9,y=2875),label=\"September, 2018\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to PR**\n",
    "\n",
    "Eventually, the paper is published in the [Journal of the American Medical Association, JAMA.](https://doi.org/10.1001/jama.2018.10929) Here is the important figure from the paper (tidied up by the good people at JAMA).<br><br>\n",
    "\n",
    "<img src=https://jamanetwork.com/data/Journals/JAMA/0/jld180037f1.png width=500>\n",
    "\n",
    "<font color=\"grey\">**Dec 05** - We receive data from a Demographic Registry demographer, but it does not include the most important part: the 2017 data. They claim that they “still do not have these data available”.\n",
    "\n",
    "**Dec 05** - We start finalizing the study design for a survey. Based on the limited information we have, we perform a power calculations and decide to make the sample size as large as our funds permit.</font>\n",
    "\n",
    "In statistical parlance the \"power\" of a statistical test is the chance that it will correctly \"detect\" a difference of a given size. This is a bit out of the scope of our discussion today but we will have a little more to say about it later.\n",
    "\n",
    "<font color=\"grey\">**Dec 06** - PSU study author replies. But email with data appears to be lost in the mail.\n",
    "\n",
    "**Dec 07** - Centro de Periodismo Investigative (CPI) publishes an [estimate of excess deaths](http://periodismoinvestigativo.com/2017/12/se-disparan-en-casi-mil-las-muertes-tras-maria/) based on September and October data of about 1,000. It appears they have 2017 data!\n",
    "\n",
    "<img src=http://periodismoinvestigativo.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-06-at-7.59.31-PM-2.png width=500>\n",
    "\n",
    "**Dec 07** – From [this tweet,](https://twitter.com/AppDemography/status/938979566282006528) it appears PSU investigator also has the data. I ask on twitter if CPI or PSU investigator have official data.\n",
    "\n",
    "**Dec 08** – I email PSU investigator asking for data.\n",
    "\n",
    "**Dec 08** - New York Times published a comprehensive article with very nice data visualization and an estimate of 1,052. They have daily data!</font>\n",
    "\n",
    "At this point, the official death toll is still 64. The Times received daily mortality data that let them estimate the impact of the hurricane. Again, this is a simple differencing technique where now they compare 2017 deaths to the average of 2016 and 2015, day by day in September and October. They call this difference \"excess deaths\" and come up with a single number of deaths to date.\n",
    "\n",
    "[Official Toll in Puerto Rico: 64. Actual Deaths May Be 1,052.](https://www.nytimes.com/interactive/2017/12/08/us/puerto-rico-hurricane-maria-death-toll.html)*\n",
    ">*A review by The New York Times of daily mortality data from Puerto Rico’s vital statistics bureau indicates a significantly higher death toll after the hurricane than the government there has acknowledged.\n",
    "<br><br>\n",
    "The Times’s analysis found that in the 42 days after Hurricane Maria made landfall on Sept. 20 as a Category 4 storm, 1,052 more people than usual died across the island. The analysis compared the number of deaths for each day in 2017 with the average of the number of deaths for the same days in 2015 and 2016.\n",
    "\n",
    "What do you think of this?\n",
    "\n",
    "<font color=\"grey\">**Dec 08** – I email first author of New York Times article. She says it took 100 emails/calls to get the data and suggest we contact the Registry director. So now we know the Demographic Registry does in fact have the data.</font>\n",
    "\n",
    "Data sharing. Editors often have difficulty knowing what to do with data sharing. This has been something that has been fueling so-called \"reproducible research\" in computational science. There are plenty of reasons not to share data, but the trend seems to be that, when possible and when it poses no harm to sources, we should be sharing data and code. A new startup journalism effort, The Markup, has a very forward looking data ethics statement to look to for an example. [The Markup Ethics Statement](https://themarkup.org/ethics.html)\n",
    "\n",
    "<font color=\"grey\">**Dec 08** – It appears that the 2017 data does exist and three different groups were given access. We were told, three days earlier, by the Demographic Registry that they “still do not have these data available”. So I email PRIS again to see if they can help.\n",
    "\n",
    "**Dec 09** – I ask Dr. Buckee to email Registry director to check that it is not just me that is being denied the data.\n",
    "\n",
    "**Dec 11** - PRIS replies. They give us the name of a Registry demographer that gave data to others. PRIS emails Registry directory, again, on our behalf.\n",
    "\n",
    "**Dec 13** – The official death count is about \\[64\\] now and Public Security Secretary dismisses current excess deaths estimates....</font>\n",
    "<br><br>\n",
    "\n",
    "<img src=https://pbs.twimg.com/media/DQ7pLJSXkAA6RC1.jpg width=500>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs <- summarize(group_by(mortality2,month), average = mean(deaths))\n",
    "\n",
    "ggplot() + \n",
    "    geom_line(aes(x=month,y=deaths,group=year),mortality2,color=\"grey\") +\n",
    "    geom_point(aes(x=9, y=2838), color=\"black\") +\n",
    "    geom_text(aes(x=9,y=2875),label=\"September, 2018\") +\n",
    "    geom_line(aes(x=month,y=average),avgs,group=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"grey\">**Jan 07** - The New York Times shares the raw data they obtained with us. It’s in PDF! But scrapable. This data shows very clearly that in September and October ther was a huge spike in deaths...</font>\n",
    "\n",
    "The official data release from Puerto Rico, obtained from the Times, was in the form of a PDF [that Rafa and his team have made available here.](https://github.com/c2-d2/pr_mort_official/raw/master/data/RD-Mortality-Report_2015-18-180531.pdf) That means we need Tabula again to extract the data we need.\n",
    "\n",
    "This is where the story takes off. So far, we have seen several different kinds of data collection, all aimed at somehow getting around the fact that pieces of the official data record is missing. Each one comes with different \"narrative value.\" As we followed this blog post, we also see Rafa building his own approach. What does he do? Why is his different?\n",
    "\n",
    "<br>\n",
    "<img src=https://github.com/cocteau/pr/raw/master/images/aa2.jpg width=600>\n",
    "<br>\n",
    "\n",
    "<font color=\"grey\">**Mar 16** - First draft of our survey paper is completed and submitted to the New England Journal of Medicine (NEJM). A particularly troubling finding is the large proportion of deaths attributed to lack of access to medical services. We also see evidence of indirect effects lasting until December 31, the end of the survey period.\n",
    "<br><br>\n",
    "**May 04** - Survey paper is tentatively accepted by NEJM.\n",
    "<br><br>\n",
    "**May 25** - Official death count is still at 64. We send a draft of our paper to PR governors office.\n",
    "<br><br>\n",
    "**May 29** – Our [NEJM](https://www.nejm.org/doi/10.1056/NEJMsa1803972) paper comes out and gets extensive media coverage: 410 outlets including articles in NPR, Washignton Post, New York Times, and CNN. Despite our best efforts, including rewriting our university’s press release, most headlines focus on the point estimate and don’t report uncertainty. All the data and code is made available on GitHub.</font>\n",
    "\n",
    "Washington Post's headline was \"Harvard study estimates thousands died in Puerto Rico because of Hurricane Maria,\" the New York Times reportes \"Puerto Rico’s Hurricane Maria Death Toll Could Exceed 4,000, New Study Estimates,\" and CNN led with \"Hurricane Maria death toll may be more than 4,600 in Puerto Rico.\" As Rafa points out, none of the headlines underscore the uncertainty in their procedure. Later the Washington Post published a fact check [\"Did 4,645 people die in Hurricane Maria? Nope.\"](https://www.washingtonpost.com/news/fact-checker/wp/2018/06/02/did-4645-people-die-in-hurricane-maria-nope/?utm_term=.66bf2eb3eaac) but misinterprets the measure of uncertainty used. FiveThirtyEight did a nice job, though with its [\"Media Reports About The Death Toll In Puerto Rico Are Needlessly Confusing\"](https://fivethirtyeight.com/features/media-reports-about-the-death-toll-in-puerto-rico-are-needlessly-confusing/).<br><br>\n",
    "\n",
    "<img src=\"https://github.com/c2-d2/pr_mort_official/raw/master/misc/faq_fig.png\" width=600><br><br>\n",
    "\n",
    "<font color=\"grey\">**May 31** - We post an [FAQ (in English and Spanish)](https://github.com/c2-d2/pr_mort_official/blob/master/misc/faq.md) explaining the uncertainty in our estimate and making it clear that our study does not say that 4645 (the point estimate) people died."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistics Interlude**\n",
    "\n",
    "The death estimates produced by various organizations (news outlets, government offices, academics) we have seen so far are very different in character. Some have been simple arithmetic calculations, others rely on formal statistical procedures. Some, like CNN's telephone census, have an \"uncertainty\" that is hard to reason around, while the survey technique used by Rafa comes with a specific, statistical notion of the uncertainty in his estimate. \n",
    "\n",
    "Rafa makes use of something called a \"confidence interval\" to assign uncertainty to the estimate of \"excess deaths\" from the hurricane. A confidence interval is a tool developed in a particular branch of statistics. Statistics, like most fields, can be roughly divided along methodological lines. In statistics, many of the distinctions stem from how you interpret perhaps the most basic of all quantities, probability. *So-called \"frequentist statistics\" defines probability as the limit of its relative frequency in a large sequence of trials.* \n",
    "\n",
    "Let's take a simple example. Suppose we want to know how often a coin, when tossed, will come up heads. You could say 50% because there's no physical reason you can think of why one side would be favored by another. This would be the \"classical\" interpretation of probability and was first applied to games of chance -- rolling dice, flipping coins, drawing cards. \n",
    "\n",
    "The frequentist would instead think about tossing that coin a number of times so that, in the limit, the probability of seeing heads would emerge. Here's a simple experiment. Toss a coin 10 times..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(url(\"https://github.com/cocteau/breakfasts/raw/master/data/coin.Rda\"))\n",
    "\n",
    "toss(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or perhaps 100 times. Then look at the proportion of heads we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toss(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've made this easy using a couple of R \"functions\" I wrote to just toss coins for you (yes, you have to believe me that a computer can do a pretty good job of simulating a fair coin toss). Here we toss 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toss_many(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening as we toss more and more coins? This table doesn't really help us. Let's have a look at a graph instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tosses <- toss_many(100)\n",
    "\n",
    "ggplot()+\n",
    "    geom_hline(yintercept=0.5,color=\"grey\")+\n",
    "    geom_line(aes(x=n,y=prop_heads),tosses)+\n",
    "    ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? Over time, the proportion of heads settles down on 50%. In this case we were tossing a fair coin so this is the right answer. What I wanted to show you was that, as a frequentist, you would think about probability as emerging from a sequence of identical trials. The relative frequency that an event occurs in this sequence tends to the probability of the event. Mind you, there are several ways of defining probability -- and you will find statisticians whose methods start with these different definitions. \n",
    "\n",
    "Now, let's go back to the excess deaths in Puerto Rico from the hurricane. Rafa is after the number of excess deaths that occured on the island due to the hurricane. Ignoring practicalities for the moment, he could really only get at this number if he interviewed every household in Puerto Rico. This *true value* means talking to the entire population. But, given limited resources, he randomly selected a fraction of the households to visit. The number of excess deaths he computes from this sample is then an *estimate* of the *true value* that he would get if he interviewed everyone. \n",
    "\n",
    "Rafa conducted a survey of 3,299 randomly selected households on the island. He asked about each member of the household and whether they had died as a result of the hurricane. [His survey is part of the NEJM supplementary material](https://www.nejm.org/doi/suppl/10.1056/NEJMsa1803972/suppl_file/nejmsa1803972_appendix.pdf). He was then able to produce an estimate of the total excess deaths attributable to the hurricane as 4,645. \n",
    ">From the survey data, we estimated a mortality rate of 14.3 deaths (95% confidence interval [CI], 9.8 to 18.9) per 1000 persons from September 20 through December 31, 2017. This rate yielded a total of 4645 excess deaths during this period (95% CI, 793 to 8498), equivalent to a 62% increase in the mortality rate as compared with the same period in 2016. However, this number is likely to be an underestimate because of survivor bias. The mortality rate remained high through the end of December 2017, and one third of the deaths were attributed to delayed or interrupted health care. Hurricane-related migration was substantial.\n",
    "\n",
    "Associated with this number is an assement of its uncertainty, *a confidence interval*. A confidence interval gives you a sense of the precision of your estimate. For any measure, we need some sense of its precision -- if only to decide if the number we've been quoted is useful to us or not. For example, a monitor that reports your heartrate to within plus or minus 20 beats per minute is not particularly informative, and certainly not useful if we are trying to track our pulse during exercise. Similarly, a tool that can report distances to within five miles might be useful for planning a car trip, but we might think twice about using it before planning a hike.\n",
    "\n",
    "Now, when given an estimate, a frequentist will express its precision through a confidence interval. Rafa is using a 95% confidence interval. The 95% is referred to as the *confidence level*, and suggests that there is some probability at work here.  And since this is a frequentist tool, that probability comes from a framework where we repeat something. In this case, we imagine Rafa repeating his experiment and randomly selecting 3,299 households again from his list on the island. His estimate of excess deaths will be different as the different households will have lost different numbers of family members. \n",
    "\n",
    "So, with a new sample he gets a new estimate and a new interval. Think about Rafa doing this over and over and over. With each new sample of households, he will get a new estimate and a new confidence interval. They will bounce around. Here's a simple simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is here just for you to execute - right now there's no reason to understand\n",
    "# it. Focus on the plot returned.\n",
    "\n",
    "n <- 10\n",
    "ed <- data.frame(trial=1:n,estimate=rnorm(n,m=4000,sd=3853/2))\n",
    "ed <- mutate(ed,cover=factor(c(\"black\",\"red\")[((estimate-3853) > 4500 | (estimate+3853) < 4500)+1]))\n",
    "\n",
    "ggplot(ed,aes(x=trial,y=estimate))+\n",
    "    geom_errorbar(aes(ymin=estimate-3853, ymax=estimate+3853))+\n",
    "    geom_point(aes(x=trial,y=estimate))+\n",
    "    theme(legend.position=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, but where does the 95% come in? Think of each survey like the toss of a coin, but this time, instead of heads or tails, the event we are looking for is whether the interval contains the true count of excess deaths or not. Again, this truth would be the number if we were able to talk to everyone on the island. And so, taking a survey, forming an estimate and building a confidence interval is like tossing a coin. Heads, if the interval contains the true value, tails otherwise. \n",
    "\n",
    "The 95% means that we have calibrated our intervals so that in 95% of the trials, they will contain this true value. That means 5% of the time, the interval will not contain the true value. If we want to be more sure of ourselves, we could form a 99% interval -- then we would be making a mistake only 1% of the time. The \"gotcha\" here is that with the extra comfort comes a wider interval -- we could be 100% certain that the number of excess deaths is between 0 and the total population of the island, for example. But that interval is so wide that we haven't learned anything.\n",
    "\n",
    "Because we are not talking to everyone on the island, we have a degree of uncertainty about how close the estimate we've computed is to the population value. The confidence interval is one way to express that uncertainty. \n",
    "\n",
    "Statistics is all about the artful application of randomness so as to be able to learn something about a phenomenon in the world. Here we are using the mechanics of a simple random sample to provide us with a framework for producing informative intervals about the plausible values for our population-level quantity, the true number of excess deaths due to the hurricane. \n",
    "\n",
    "Here is our small experiment from above, this time adding the true population value and highlighting which intervals do not contain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(ed,aes(x=trial,y=estimate,colour=cover))+\n",
    "    scale_color_manual(\"cover\", breaks=c(1,2),values=c(\"#000000\", \"#FF0000\"))+\n",
    "    geom_errorbar(aes(ymin=estimate-3853, ymax=estimate+3853,color=cover))+\n",
    "    geom_point(aes(x=trial,y=estimate,color=cover))+\n",
    "    geom_hline(yintercept=4500,color=\"blue\")+\n",
    "    theme(legend.position=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- 100\n",
    "ed <- data.frame(trial=1:n,estimate=rnorm(n,m=4000,sd=3853/2))\n",
    "ed <- mutate(ed,cover=factor(c(\"black\",\"red\")[((estimate-3853) > 4500 | (estimate+3853) < 4500)+1]))\n",
    "\n",
    "ggplot(ed,aes(x=trial,y=estimate,colour=cover))+\n",
    "    scale_color_manual(\"cover\", breaks=c(1,2),values=c(\"#000000\", \"#FF0000\"))+\n",
    "    geom_errorbar(aes(ymin=estimate-3853, ymax=estimate+3853,color=cover))+\n",
    "    geom_point(aes(x=trial,y=estimate,color=cover))+\n",
    "    geom_hline(yintercept=4500,color=\"blue\")+\n",
    "    theme(legend.position=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small problem with this confidence interval idea is that it actually can't tell us whether the true value we're after is in a given interval or not. It simply says that the procedure tends to produce intervals that cover the truth 95% of the time. But we could be unlucky, and 1 in 20 times, we will be staring at an interval that does not contain the true value... and won't know it.\n",
    "\n",
    "The width of a confidence interval depends on the number of data points we are working with (how many households Rafa talked to) and the confidence level (and possibly on the unknown population values as well). In general, the width drops the with more points, but widens as we demand greater confidence. Experimenters will set the confidence level after careful thought (\"What kind of error can I tolerate?\") or will have it set for them by a publication as a precondition of publication. \n",
    "\n",
    "Sample size, on the other hand, is often a tradeoff between how much time and money an experimenter has, and some prior assessment of how narrow they need the intervals to be to detect the effect they are looking for. Remember the example of a heartrate monitor that reported your pulse to within plus or minus 20 beats per minute. If you want to make more refined judgements, you need a narrower interval. This suggests a tradeoff between precision and cost. \n",
    "\n",
    "(If you are interested, the concept to follow up on is called *power* and is usually worded in terms of statistical testing.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to PR**\n",
    "\n",
    "<font color=\"grey\">**Jun 01** – PR Governor is interviewed by CNN’s Anderson Cooper who grills him on why the government did not share the data. Governor says data was always available and that there will be “hell to pay if data not available”. That afternoon, PR government makes data public. Dr. Buckee requests the data again. This time we get it almost immediately.\n",
    "<br><br>\n",
    "**Jan 06** - I post my first analysis with the official mortality data here. The newly released data confirms that the data we were provided earlier were in fact incomplete and that there was a sustained indirect effect lasting past October.\n",
    "<br><br>\n",
    "**Aug 27** - GWU study comes out with a preliminary estimate of about 3,000 deaths.\n",
    "<br><br>\n",
    "**Sep 05** - Rolando Acosta and I post a preprint describing an analysis of the newly released data including a comparison to the effects of other hurricanes. We also provide a preliminary estimate of about 3,000 deaths due to indirect effects lasting until April.</font>\n",
    "\n",
    "I will leave it to you to discuss the GWU study and their methodology. What was followed? Is it definitive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The official record ###\n",
    "<img src=https://datos.estadisticas.pr/uploads/group/2018-08-24-155513.166294regdem.jpg width=100 align=left><BR CLEAR=\"LEFT\"> \n",
    "\n",
    "Let's leave Rafa's blog for now and consider the status of the official data. It was clearly under tight control. While academics and journalists alike struggled to get a handle on the actual death toll, the need to see the official record was growing. CNN and the Center for Investigative Journalism sued for access to these records and on June 5, 2018, a judge ordered the government to release death certificates. \n",
    "\n",
    "[Puerto Rico ordered to release death records](https://www.cnn.com/2018/06/05/us/puerto-rico-hurricane-maria-death-records/index.html)\n",
    "\n",
    ">The Puerto Rican government has seven days to release death certificates and related data to CNN and a local journalism organization investigating the true toll of Hurricane Maria, a Puerto Rican judge ruled Monday... <br><br>\n",
    "CNN and the Center for Investigative Journalism (CPI) in Puerto Rico sued for access to death records. Both organizations have published extensive investigations questioning Puerto Rico's official death toll, which is 64.<br><br>\n",
    "Data about Hurricane Maria deaths has been the Puerto Rican government's \"best kept secret,\" with officials blocking journalists and academics from obtaining basic mortality data, said Carla Minet, executive director of the Center for Investigative Journalism in Puerto Rico.\n",
    "\n",
    "Perhaps not surprisingly in the 11th hour things stalled.\n",
    "\n",
    "[Puerto Rico seeks to delay releasing death records after hurricane; judge rejects motion](https://www.cnn.com/2018/06/12/politics/sutter-puerto-rico-records-delay-invs/index.html)\n",
    ">Puerto Rico Gov. Ricardo Rosselló told CNN last month that there would be \"hell to pay\" if officials withheld records related to Hurricane Maria.\n",
    "<br><br>\n",
    "Yet his government filed a motion late Monday asking for permission **to stall the delivery of death certificates and other data** a judge had ordered to be released to CNN and the Center for Investigative Journalism, or CPI, after the organizations sued for access to those records.<br><br>\n",
    "In response, the court said it would not lift its requirement that records be released to CNN and CPI by Tuesday.\n",
    "\n",
    "Eventually, the death certificates were released. Both CNN and CPI made the data available to the public, creating searchable databases. Again, publishing data is a political act. It also has clear ethical dimensions. Compare the way both organizations published their results. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[CNN: Help CNN investigate hurricane-related deaths in Puerto Rico](https://www.cnn.com/interactive/2018/06/health/puerto-rico-deaths-invs/)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[CPI: Hurricane Maria’s victims](https://hurricanemariasdead.com/database.html)\n",
    "\n",
    "Tell me about the differences. As an editor, which would you publish? Why? Think back to our discussion about data sharing.\n",
    "\n",
    "**Having a look at the data**\n",
    "\n",
    "The Board of Directors for Puerto Rico's Institute of Statistics passed a resolution to make data like these available more readily to the public through their open data portal. \n",
    ">To approve the methodology to determine if a death was related to a disaster, so that the\n",
    "statistics of these deaths are reliable, comparable, and accessible.\n",
    "\n",
    "You can [read the full resolution](https://estadisticas.pr/files/nosotros/resoluciones/Resolucion_2018-03_EI.pdf) signed April 24, 2018. Evidently, the health department had issues sharing this data, and after CNN's successful suit, Puerto Rico's Institute of Statistics also sued the department for the data. This was June 1, 2018.\n",
    "\n",
    "[Puerto Rico agency sues government to obtain death data](https://apnews.com/7d19e956de344d7188c79b455c0315f6)\n",
    "\n",
    ">SAN JUAN, Puerto Rico (AP) — Puerto Rico’s Institute of Statistics announced Friday that it has sued the U.S. territory’s health department and demographic registry seeking to obtain data on the number of deaths following Hurricane Maria as a growing number of critics accuse the government of lacking transparency.\n",
    "<br><br>\n",
    "The lawsuit was filed Thursday, the same day Puerto Rico Gov. Ricardo Rossello told CNN there would be “hell to pay” if officials don’t release mortality data. Puerto Rico’s Health Department released some information Friday, saying an additional 1,397 overall deaths were reported from September to December in 2017, compared with the same period the previous year. However, officials did not provide causes of death for any of the 11,459 total people deceased during the period.\n",
    "\n",
    "Eventually the data appeared and are now being published in what is claimed to be a sustainable way (even if the electricity goes out, we are told). You can visit \n",
    "[the open data portal](https://estadisticas.pr/en/media/3176) and see the data they provide about deaths since January 1, 2018. What can you tell me about the data? How is it published? \n",
    "\n",
    "https://datos.estadisticas.pr/dataset/defunciones-registradas-hasta-la-semana-pasada\n",
    "\n",
    ">Deaths registered until last week\n",
    "File of each death registered in Puerto Rico from January 1, 2017 until last week. Some deaths can be registered weeks, months and even years after they occur, so users should bear in mind that although data from last week are offered, in all probability the deaths that occurred last week are not included: only the deaths that occurred they came to register last week. For each death (row), information is included about causes of death (ICD-10), year and month of death, place of residence, place of birth, type of death, sex, marital status, year of birth, age , level of education, occupation, industry, veteran status, among others. This set of data is prepared to comply with Resolution No. 2018-03 of the Board of Directors of the Institute. <br>\\[Google translation\\]\n",
    "\n",
    "For what it's worth, API stands for \"Application Programming Interface\" and is a popular way for people to expose data. Rather than store data in a single file like a CSV, API's usually let you specify the data you want. They serve data in response to requests. API's became popular in a bygone age of \"mash-ups\" when the output of one service could be fed into another. This interoperability implied the need to share data, meaning data had to be formatted for machines to easily make sense of it. PDF and HTML are not part of this world as you have to work too hard and it can be a manual process. CSV's are OK. Formats like XML (a sexier version of HTML where the tags denote features of the data) or JSON (Javascript Object Notation) are the typical data formats associated with APIs. And lots of services offer APIs - Twitter, Facebook, YouTube...\n",
    "\n",
    "You can play with XML, JSON and CSV say using Shan Carter's [Mr. Data Converter](https://shancarter.github.io/mr-data-converter/).\n",
    "\n",
    "Now, let's read in the mortality data and make some graphs. Make sure to consult the [data dictionary](https://datos.estadisticas.pr/dataset/defunciones-registradas-hasta-la-semana-pasada/resource/5d9b5d1b-d0f8-4155-8a80-b03ff2c2dff9) so you know what each column represents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore - just setting things to work with spanish and to \n",
    "#display all 65 columns of the table\n",
    "loc <- Sys.setlocale('LC_ALL','C')\n",
    "options(repr.matrix.max.cols=65)\n",
    "\n",
    "# now read in the data from the official site and have a look\n",
    "official <- read.csv(\"https://datos.estadisticas.pr/dataset/bc0de091-b513-4b48-bfaa-c238522e180a/resource/5d9b5d1b-d0f8-4155-8a80-b03ff2c2dff9/download/regdem-defunciones-01-enero-2017-hasta-23-noviembre-2018.csv\")\n",
    "head(official)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(official)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a new data set, I often just want to have a look. This is a back-and-forth process with the data dictionary. What is the variable then what are its values? I usually like to divide data into two types - **quantitative measures** that represent numerical values (like counts or measurements from sensors, say), and then **qualitative measures** that are like categories (like gender or city).  This exercise is useful because it tells you what does and does not make sense to do to the data. \n",
    "\n",
    "For example, Zip code. It's qualitative and so it doesn't make sense to take an average of a column of them. When we have categories, the easiest kind of summary statistic is just a frequency display - a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen <- summarize(group_by(official,Gender),n=n())\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=10, repr.plot.height=5)\n",
    "\n",
    "ggplot() + geom_bar(aes(x=Gender),official) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot() + geom_bar(aes(x=MaritalStatus),official) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrange(summarize(group_by(official,BirthPlace),n=n()),desc(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quantitative variables we also want to look at the shape of the data. There are stories in multiple clusters or modes. There are stories in outlying points. Age is the first quantitative variable in the data set and let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot() + geom_histogram(aes(x=Age),official,bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n(official,5,Age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot() + geom_histogram(aes(x=Age),filter(official,Age<999),col=\"white\",bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot() + geom_histogram(aes(x=Age),filter(official,Age<999),col=\"white\",bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distribution is said to be symmetric if its left and right halves are the same. The mean and standard deviation are well suited for symmetric (and well-behaved) distributions. If, on the other hand, you have skew like this distribution, then the median and IQR might provide  better summaries. In choosing to reduce a data set to one or two numbers, you probably also have to consider the communities of practice around the data you're looking at. We often talk about median home prices because of the skew problem with the mean, for example. So if you were to write about real estate, the summary statistic has been chosen for you. (Not that you shouldn't question how things are done.)\n",
    "\n",
    "To examine this, let's look more closely at the monthly data. What do you think about December and January in these plots? Look at the 7 numbers for each month. What can you say? To make these observations more quantitative, we might look at the \"spread\" associated with each month. One measure of spread (we'll talk about this more shortly) is the so-called standard deviation. The average and standard deviation are two summary statistics that are meant to capture the center and spread of the data respectively.\n",
    "\n",
    "Let's remake our plot, adding an average and then make a plot with the standard deviations. What do you see? Recall that the mean or average of a the numbers for 7 years in our data set is given by\n",
    "\n",
    "$$\\mbox{avg} = \\frac{y_1 + y_2 + y_3 + \\cdots +y_{n-2} + y_{n-1} + y_n}{n} $$\n",
    "\n",
    "*While the so-called variance of these numbers is...*\n",
    "\n",
    "$$\\mbox{var} = \\frac{(y_1-\\mbox{avg})^2 + (y_2-\\mbox{avg})^2 + (y_3-\\mbox{avg})^2 + \\cdots +(y_{n-2}-\\mbox{avg})^2 + (y_{n-1}-\\mbox{avg})^2 + (y_n-\\mbox{avg})^2}{6} $$\n",
    "\n",
    "... which records how far each point is from the average. You can show that if you consider any other point as the average of a data set, this total distance to that point can't be lower than that of the mean. It's in this sense that we can think of the average as being the \"center\" of a data set. The variance, then, measures how spread out the data are around the center point, the average. It is common to quote this spread not in terms of the variance but its square root, called the standard deviation.\n",
    "\n",
    "$$\\mbox{sd} = \\sqrt{\\mbox{var}}$$\n",
    "\n",
    "Mean and standard deviation measure the center and spread. They work hand in glove with the so-called normal distribution. The \"bell curve,\" it is symmetric and takes on a very specific form. To draw these curves we just need to know the mean and standard deviation -- where is the peak located and how spread out is the bell. Mean and standard deviation are paired with distributions that look like this. If you get data with a different shape, you should consider possibly other summary statistics when telling your story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One normal\n",
    "pts1 <- seq(-4,4,len=100)\n",
    "norm1 <- dnorm(pts1)\n",
    "\n",
    "#Another normal\n",
    "pts2 <- seq(0,4,len=100)\n",
    "norm2 <- dnorm(pts2,m=1,sd=0.5)\n",
    "\n",
    "#Another normal\n",
    "pts3 <- seq(-2,2,len=100)\n",
    "norm3 <- dnorm(pts3,m=0.5,sd=0.25)\n",
    "\n",
    "ggplot()+geom_line(aes(x=pts1,y=norm1))+\n",
    "    geom_line(aes(x=pts2,y=norm2),color=\"orange\")+\n",
    "    geom_line(aes(x=pts3,y=norm3),color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and standard deviation are not the only summary statistics that capture the center and spread of a data set. A \"robust\" pair of alternatives are the mean and so-called inter-quartile range. A mouthful, yes. But when we appeal to these the \"center\" plot looks about the same, while our view of the variability is somewhat changed.\n",
    "\n",
    "If you have an odd number of points, the median is literally the point in the middle of the data set. The upper quartile, median and lower quartile divide the data into 4 parts. 25% of the data is below the lower quartile, 25% between the lower and median, 25% between the median and the upper quartile, and 25% above the upper quartile. So the median is brute force measure of the center of a data set and by taking the difference between the upper and lower quartiles, we see how spread out the data are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(filter(official,Age<999),avg=mean(Age),sd=sd(Age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summarize(filter(official,Age<999),lq=quantile(Age,0.25),med=median(Age),uq=quantile(Age,0.75),iqr=IQR(Age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts <- seq(35,110,len=500)\n",
    "norm <- dnorm(pts,m=73.7,sd=17.5)\n",
    "\n",
    "ggplot() + \n",
    "    geom_histogram(aes(x=Age,y = ..density..),filter(official,Age<999),col=\"white\",,bins=30)+ \n",
    "    geom_line(aes(x=pts,y=norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot() + \n",
    "    geom_histogram(aes(x=Age),filter(official,Age<999),col=\"white\",bins=30)+ \n",
    "    geom_vline(xintercept=65,color=\"green\")+\n",
    "    geom_vline(xintercept=86,color=\"green\")+\n",
    "    geom_vline(xintercept=77,color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although you will never see one in a publication, the time-tested boxplot incorporates all the \"robust\" measures we've been talking about - the median and the upper and lower quartiles. They provide a handy thumbnail to compare distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "\n",
    "ggplot() + \n",
    "    geom_boxplot(aes(x=Gender,y=Age),filter(official,Age<999 & Gender != \"U\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of these displays taken from Rafa's NEJM paper. With his survey, he tried to also study the conditions on the ground that might have caused the high death toll. He examined by \"remoteness\" (recall the map above) as well as various specific causes that might hinder medical assistance. Qualitative variables summarized with a (horizontal) bar chart and boxplots to compare distributions by remoteness. What do you see?\n",
    "\n",
    "<img src=https://www.nejm.org/na101/home/literatum/publisher/mms/journals/content/nejm/2018/nejm_2018.379.issue-2/nejmsa1803972/20180716-01/images/img_xlarge/nejmsa1803972_f3.jpeg width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative, we can \"facet\" our histogram and divide the picture into two categories - one for males and one for females. This also allows for direct comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=10, repr.plot.height=5)\n",
    "\n",
    "ggplot() + \n",
    "    geom_histogram(aes(x=Age),filter(official,Age<999 & Gender!=\"U\"),col=\"white\",bins=50) + \n",
    "    facet_grid(Gender ~ .)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having access to the official records seems like a huge win. We now have the records of everyone who died during the time period we're interested in. There is a cause of death denoted \"Victim of cataclysmic storm\" (the [CDC has a precise definition](https://www.cdc.gov/nchs/data/nvss/vsrg/vsrg01.pdf)) that we can start to work through. CNN's Sutter started there and found a mismatch between the official list of those killed in the storm and those in the records listed as dying from a cataclysmic storm - [Their deaths were labeled 'victim of cataclysmic storm.' So why aren't their names on Puerto Rico's list?](https://www.cnn.com/2018/06/16/health/cataclysmic-storm-deaths-puerto-rico-maria-invs/index.html?no-st=1538964183). In addition, Sutter found out that there are plenty of other causes of death that can be linked to the storm - [Deaths from bacterial disease in Puerto Rico spiked after Maria](https://www.cnn.com/2018/07/03/health/sutter-leptospirosis-outbreak-puerto-rico-invs/index.html). \n",
    "\n",
    "At this point we can start to use the codes or even the text of the cause of death to start to assess the actual death toll. It is likely that this number will never be known and at best we can guess at the order of magnitude. My hope by presenting this narrative is that you see that there are many different kinds of data that you can bring to a question, each with thier own peculiarities. The journalism from data is a wildly creative exercise and extends from data collection to analysis.\n",
    "\n",
    "Now, from here, go read the [GWU study](https://publichealth.gwu.edu/sites/default/files/downloads/projects/PRstudy/Acertainment%20of%20the%20Estimated%20Excess%20Mortality%20from%20Hurricane%20Maria%20in%20Puerto%20Rico.pdf) commissioned by the government of Puerto Rico. What methods were used? What data is available? What kinds of arguments were made?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On your own###\n",
    "\n",
    "Finally, if you want to work a little on your own, here is the CDC BRFSS data for Puerto Rico from 2017. Each row is a respondent (we have taken a subsample of 1000), and the variables (columns) are sex, age in 5 year groups, height, weight, a measure of the person's general health, whether they have a health plan, whether they have had difficulty dressing or walking in the last 30 days, their education level, employment status and income. Which variables are qualitative? Which are quantitative? Tell me a story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(url(\"https://github.com/cocteau/breakfasts/raw/master/data/cdc.Rda\"))\n",
    "head(cdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
